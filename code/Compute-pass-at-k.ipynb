{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0225beef",
   "metadata": {},
   "source": [
    "This notebook computes the pass@k score across k result dataframes provided as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a4257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93be075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pass_rate(df_list, rows):\n",
    "    \"\"\"\n",
    "    Calculate the maximum pass rate and corresponding index for each row across multiple dataframes.\n",
    "    Pass rate is defined as the number of tests passed for each code segment.\n",
    "    Args:\n",
    "      df_list (list): List of dataframes to compare.\n",
    "      rows (int): Number of rows in each dataframe.\n",
    "    Returns:\n",
    "      max_pass_rate_list (list): List of maximum pass rates for each row.\n",
    "      max_index_list (list): List of indices corresponding to the maximum pass rates.\n",
    "    \"\"\"\n",
    "    max_pass_rate_list = []\n",
    "    max_index_list = []\n",
    "    for i in range(rows):\n",
    "        max_pass_rate = 0\n",
    "        max_index = 0\n",
    "        for j in range(len(df_list)):\n",
    "            pass_rate = df_list[j]['Total Passed'].iloc[i]\n",
    "            max_pass_rate = max(max_pass_rate, pass_rate)\n",
    "            if max_pass_rate == pass_rate:\n",
    "                max_index = j\n",
    "        max_pass_rate_list.append(max_pass_rate)\n",
    "        max_index_list.append(max_index)\n",
    "    return max_pass_rate_list, max_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c05f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution(df):\n",
    "    \"\"\"\n",
    "    Calculate the distribution of test results across the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "      dist (dict): A dictionary containing the distribution of test results.\n",
    "    \"\"\"\n",
    "    dist = {}\n",
    "    for col in df.columns:\n",
    "        if col in ['text', 'GT Code', 'Generated Code', 'Total Passed', 'True Positives', 'False Positives', 'False Negatives', 'Accuracy', 'Recall', 'Precision', 'Mismacthes']:\n",
    "            continue\n",
    "        counts = df[col].value_counts().to_dict()\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        passed = 0\n",
    "        for key, value in counts.items():\n",
    "            key = [int(k) for k in key[1:-1].split(', ')]\n",
    "            if key[0]==1:\n",
    "                passed += value\n",
    "            if key[0]==1 and key[1]==1:\n",
    "                tp += value\n",
    "            elif key[0]==0 and key[1]==0:\n",
    "                fp += value\n",
    "            elif key[0]==0 and key[1]==1:\n",
    "                fn += value\n",
    "        acc = passed / len(df)\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "        dist[col] = { 'Accuracy': acc, 'Recall': recall, 'Precision': precision }\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e177a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_from_result_df(df_result):\n",
    "    \"\"\"\n",
    "    Calculate the overall metrics from the results dataframe.\n",
    "    \n",
    "    Returns:\n",
    "      mean_accuracy (float): Average accuracy over all rows.\n",
    "      mean_recall (float): Average recall over all rows.\n",
    "      mean_precision (float): Average precision over all rows.\n",
    "    \"\"\"\n",
    "    n = len(df_result)\n",
    "    total_accuracy = df_result['Accuracy'].sum()\n",
    "    total_recall = df_result['Recall'].sum()\n",
    "    total_precision = df_result['Precision'].sum()\n",
    "    \n",
    "    mean_accuracy = total_accuracy / n if n else 0\n",
    "    mean_recall = total_recall / n if n else 0\n",
    "    mean_precision = total_precision / n if n else 0\n",
    "    dist = calculate_distribution(df_result)\n",
    "    \n",
    "    return mean_accuracy, mean_recall, mean_precision, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f492758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 9\n",
      "Number of rows where all tests passed: 3\n",
      "Percentage of rows where all tests passed (pass@3): 33.33%\n"
     ]
    }
   ],
   "source": [
    "k = 3 # Update this to the value of k you want to use\n",
    "\n",
    "# Read the k CSV files into dataframes (sample paths for k=3 shown)\n",
    "df1 = pd.read_csv('path/to/your/csvfile1.csv')\n",
    "df2 = pd.read_csv('path/to/your/csvfile2.csv')\n",
    "df3 = pd.read_csv('path/to/your/csvfile3.csv')\n",
    "\n",
    "# Assuming the dataframes have a consistent structure, concatenate them into a list\n",
    "df_list = [df1, df2, df3]\n",
    "\n",
    "# Number of rows in each dataframe\n",
    "rows = len(df1)\n",
    "\n",
    "total_tests = 16 # Total number of semantic tests (fixed value)\n",
    "\n",
    "# Calculate the pass rates and indices\n",
    "max_pass_rate_list, max_index_list = calculate_pass_rate(df_list, rows)\n",
    "\n",
    "# Calculating number of rows where all tests passed\n",
    "all_passed = 0\n",
    "for i in range(rows):\n",
    "    if max_pass_rate_list[i] == total_tests:\n",
    "        all_passed+=1\n",
    "print(f\"Total number of rows: {rows}\")\n",
    "print(f\"Number of rows where all tests passed: {all_passed}\")\n",
    "print(f\"Percentage of rows where all tests passed (pass@{k}): {all_passed/rows*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76b2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results dataframe consolidating the best results\n",
    "df_consolidated = []\n",
    "for i in range(rows):\n",
    "    df_consolidated.append(df_list[max_index_list[i]].iloc[i])\n",
    "df_consolidated = pd.DataFrame(df_consolidated)\n",
    "df_consolidated.head()\n",
    "\n",
    "df_consolidated.to_csv('path/to/your/consolidated_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c413b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8264\n",
      "Mean Recall: 0.7963\n",
      "Mean Precision: 0.7685\n",
      "Distribution of test results:\n",
      "Information Description {'Accuracy': 1.0, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Definition Term {'Accuracy': 1.0, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Definition Meaning {'Accuracy': 1.0, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Definition Exclusions {'Accuracy': 0.8888888888888888, 'Recall': 0, 'Precision': 0}\n",
      "Rule Entity {'Accuracy': 0.7777777777777778, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Rule Type {'Accuracy': 0.7777777777777778, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Rule Description {'Accuracy': 0.6666666666666666, 'Recall': 1.0, 'Precision': 1.0}\n",
      "Rule Conditions {'Accuracy': 0.6666666666666666, 'Recall': 1.0, 'Precision': 0.5}\n",
      "Exemption Description {'Accuracy': 0.8888888888888888, 'Recall': 0.0, 'Precision': 0}\n",
      "Refines {'Accuracy': 0.6666666666666666, 'Recall': 0.0, 'Precision': 0}\n",
      "Is Refined By {'Accuracy': 0.7777777777777778, 'Recall': 0.0, 'Precision': 0}\n",
      "Follows {'Accuracy': 0.7777777777777778, 'Recall': 0.6666666666666666, 'Precision': 0.6666666666666666}\n",
      "Is Followed By {'Accuracy': 0.8888888888888888, 'Recall': 0, 'Precision': 0}\n",
      "Exceptions {'Accuracy': 0.8888888888888888, 'Recall': 0, 'Precision': 0}\n",
      "Is Exception To {'Accuracy': 0.7777777777777778, 'Recall': 0, 'Precision': 0.0}\n",
      "References {'Accuracy': 0.7777777777777778, 'Recall': 0.5, 'Precision': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics from the consolidated results dataframe\n",
    "mean_acc, mean_rec, mean_prec, dist = calculate_metrics_from_result_df(df_consolidated)\n",
    "print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
    "print(f\"Mean Recall: {mean_rec:.4f}\")\n",
    "print(f\"Mean Precision: {mean_prec:.4f}\")\n",
    "print(\"Distribution of test results:\")\n",
    "for key, value in dist.items():\n",
    "    print(key, value)\n",
    "dist = pd.DataFrame(dist)\n",
    "dist.to_csv('path/to/your/distribution_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f63ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
