{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes the code to run structural tests for a given test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from class_structure import *\n",
    "from serialize import *\n",
    "import unittest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_string(code_str: str) -> List[Section]:\n",
    "    \"\"\"\n",
    "    Executes the given code string in a fresh namespace and\n",
    "    returns a list of all Section objects created by that code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        namespace = {}\n",
    "        # Execute the code in an isolated namespace\n",
    "        exec(code_str, globals(), namespace)\n",
    "\n",
    "        # Collect all variables in 'namespace' that are Section instances\n",
    "        sections = [\n",
    "            value\n",
    "            for value in namespace.values()\n",
    "            if isinstance(value, Section)\n",
    "        ]\n",
    "        return sections\n",
    "    except (SyntaxError, Exception):\n",
    "        return []\n",
    "    \n",
    "def compare_strings_with_threshold(s1, s2, threshold=10):\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein (edit) distance between two strings and compare it against a threshold.\n",
    "    \n",
    "    Parameters:\n",
    "      s1 (str): First string.\n",
    "      s2 (str): Second string.\n",
    "      threshold (int): Maximum allowable edit distance to consider the strings similar.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (bool, int) where the bool is True if the edit distance is <= threshold,\n",
    "             and the int is the computed edit distance.\n",
    "    \"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    \n",
    "    for i in range(m+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n+1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i-1][j] + 1,      # deletion\n",
    "                dp[i][j-1] + 1,      # insertion\n",
    "                dp[i-1][j-1] + cost  # substitution\n",
    "            )\n",
    "    \n",
    "    edit_distance = dp[m][n]\n",
    "    return (edit_distance <= threshold, edit_distance)\n",
    "\n",
    "# compare list of strings with threshold\n",
    "def compare_list_strings_with_threshold(list1, list2, threshold=10):\n",
    "    \"\"\"\n",
    "    Compare two lists of strings using the edit distance.\n",
    "    \n",
    "    Parameters:\n",
    "      list1 (list): First list of strings.\n",
    "      list2 (list): Second list of strings.\n",
    "      threshold (int): Edit distance threshold for string comparisons.\n",
    "      \n",
    "    Returns:\n",
    "      bool: True if every corresponding pair of strings are considered equal; False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "    \n",
    "    for s1, s2 in zip(list1, list2):\n",
    "        similar, _ = compare_strings_with_threshold(s1, s2, threshold)\n",
    "        if not similar:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def compare_serialized_expr(expr1, expr2, threshold=10):\n",
    "    \"\"\"\n",
    "    Recursively compare two serialized Expression dictionaries.\n",
    "    \n",
    "    The dictionaries are assumed to have at least the following keys:\n",
    "      - \"text\": a string (already lower-cased)\n",
    "      - \"includes\": a list of nested serialized expressions or statements\n",
    "      - \"sectionNumber\": an integer (or None)\n",
    "    \n",
    "    For string values, the comparison is based on the edit distance.\n",
    "    For lists, elements are compared in order recursively.\n",
    "    Other values are compared using standard equality.\n",
    "    \n",
    "    Parameters:\n",
    "      expr1 (dict): First serialized expression.\n",
    "      expr2 (dict): Second serialized expression.\n",
    "      threshold (int): Edit distance threshold for string comparisons.\n",
    "      \n",
    "    Returns:\n",
    "      bool: True if expr1 and expr2 are considered equal under these rules; False otherwise.\n",
    "    \"\"\"\n",
    "    # Check that both dictionaries have the same keys.\n",
    "    if set(expr1.keys()) != set(expr2.keys()):\n",
    "        return False\n",
    "\n",
    "    # Compare \"text\" using edit distance.\n",
    "    if \"text\" in expr1 and \"text\" in expr2:\n",
    "        if isinstance(expr1[\"text\"], str) and isinstance(expr2[\"text\"], str):\n",
    "            similar, _ = compare_strings_with_threshold(expr1[\"text\"], expr2[\"text\"], threshold)\n",
    "            if not similar:\n",
    "                return False\n",
    "        else:\n",
    "            if expr1[\"text\"] != expr2[\"text\"]:\n",
    "                return False\n",
    "\n",
    "    # Compare \"sectionNumber\" using edit distance.\n",
    "    if \"sectionNumber\" in expr1 and \"sectionNumber\" in expr2:\n",
    "            similar, _ = compare_strings_with_threshold(str(expr1[\"sectionNumber\"]), str(expr2[\"sectionNumber\"]), threshold)\n",
    "            if not similar:\n",
    "                return False\n",
    "\n",
    "    # Compare \"includes\" recursively.\n",
    "    if \"includes\" in expr1 and \"includes\" in expr2:\n",
    "        list1 = expr1[\"includes\"]\n",
    "        list2 = expr2[\"includes\"]\n",
    "        if len(list1) != len(list2):\n",
    "            return False\n",
    "        for item1, item2 in zip(list1, list2):\n",
    "            # If the items are dictionaries, assume they are serialized expressions/statements.\n",
    "            if isinstance(item1, dict) and isinstance(item2, dict):\n",
    "                if not compare_serialized_expr(item1, item2, threshold):\n",
    "                    return False\n",
    "            # Otherwise, if they are strings, use the string comparison.\n",
    "            elif isinstance(item1, str) and isinstance(item2, str):\n",
    "                similar, _ = compare_strings_with_threshold(item1, item2, threshold)\n",
    "                if not similar:\n",
    "                    return False\n",
    "            else:\n",
    "                # For any other types, use direct equality.\n",
    "                if item1 != item2:\n",
    "                    return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_section_number(self, code_string1, code_string2):\n",
    "    \"\"\"\n",
    "    Test the section number of two code snippets.\n",
    "    This function runs both code snippets, serializes the resulting sections,\n",
    "    and compares the section numbers to ensure they match within a threshold.\n",
    "    Parameters:\n",
    "      code_string1 (str): The first code snippet as a string.\n",
    "      code_string2 (str): The second code snippet as a string.\n",
    "    Returns:\n",
    "      int: 1 if the test passes, 0 if it fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Run both snippets to get top-level sections\n",
    "        list_s1 = run_code_string(code_string1)\n",
    "        list_s2 = run_code_string(code_string2)\n",
    "\n",
    "        # 2. Serialize them\n",
    "        dict1 = [serialize_section(s) for s in list_s1]\n",
    "        dict2 = [serialize_section(s) for s in list_s2]\n",
    "        # compare length of dictionaries\n",
    "        self.assertEqual(len(dict1), len(dict2), \"The two code snippets did not produce the same number of sections.\")\n",
    "\n",
    "        for i in range(len(dict1)):\n",
    "            # 3. Compare\n",
    "            assert compare_list_strings_with_threshold([d['sectionNumber'] for d in dict1], [d['sectionNumber'] for d in dict2], threshold=10), \"The two code snippets did not produce the same section number\"\n",
    "        print(\"Section number test case passed\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Section number test case failed\")\n",
    "        return 0\n",
    "\n",
    "def test_section_name(self, gt_code, generated_code):\n",
    "    \"\"\"\n",
    "    Test the section names of two code snippets.\n",
    "    This function runs both code snippets, serializes the resulting sections,\n",
    "    and compares the section names to ensure they match within a threshold.\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      generated_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "      int: 1 if the test passes, 0 if it fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Run both snippets to get top-level sections\n",
    "        list_s1 = run_code_string(gt_code)\n",
    "        list_s2 = run_code_string(generated_code)\n",
    "\n",
    "        # 2. Serialize them\n",
    "        dict1 = [serialize_section(s) for s in list_s1]\n",
    "        dict2 = [serialize_section(s) for s in list_s2]\n",
    "\n",
    "        # compare length of dictionaries\n",
    "        self.assertEqual(len(dict1), len(dict2), \"The two code snippets did not produce the same number of sections.\")\n",
    "\n",
    "        for i in range(len(dict1)):\n",
    "            # 3. Compare\n",
    "            assert compare_list_strings_with_threshold([d['sectionTitle'] for d in dict1], [d['sectionTitle'] for d in dict2], threshold=10), \"The two code snippets did not produce the same section name\"\n",
    "        print(\"Section name test case passed\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(\"The two code snippets did not produce the same structure.\")\n",
    "        print(e)\n",
    "        print(\"Section name test case failed\")\n",
    "        return 0\n",
    "\n",
    "def test_section_subsections(self, gt_code, generated_code):\n",
    "    \"\"\"\n",
    "    Test the subsections of two code snippets.\n",
    "    This function runs both code snippets, serializes the resulting sections,\n",
    "    and compares the number of subsections in each section to ensure they match.\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      generated_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "        int: 1 if the test passes, 0 if it fails.\n",
    "        \"\"\"\n",
    "    try:\n",
    "        # 1. Run both snippets to get top-level sections\n",
    "        list_s1 = run_code_string(gt_code)\n",
    "        list_s2 = run_code_string(generated_code)\n",
    "\n",
    "        # 2. Serialize them\n",
    "        dict1 = [serialize_section(s) for s in list_s1]\n",
    "        dict2 = [serialize_section(s) for s in list_s2]\n",
    "        print(dict1)\n",
    "        print(dict2)\n",
    "\n",
    "        # compare length of dictionaries\n",
    "        self.assertEqual(len(dict1), len(dict2), \"The two code snippets did not produce the same number of sections.\")\n",
    "\n",
    "        for i in range(len(dict1)):\n",
    "            # 3. Compare number of subsections\n",
    "            self.assertEqual(len(dict1[i]['subSections']), len(dict2[i]['subSections']), \"The two code snippets did not produce the same number of subsections.\")\n",
    "        print(\"Subsection test case passed\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Subsection test case failed\")\n",
    "        return 0\n",
    "\n",
    "def test_section_expressions(self, gt_code, generated_code):\n",
    "    \"\"\"\n",
    "    Test the expressions of two code snippets.\n",
    "    This function runs both code snippets, serializes the resulting sections,\n",
    "    and compares the number of expressions in each section to ensure they match.\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      generated_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "        int: 1 if the test passes, 0 if it fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Run both snippets to get top-level sections\n",
    "        list_s1 = run_code_string(gt_code)\n",
    "        list_s2 = run_code_string(generated_code)\n",
    "\n",
    "        # 2. Serialize them\n",
    "        dict1 = [serialize_section(s) for s in list_s1]\n",
    "        dict2 = [serialize_section(s) for s in list_s2]\n",
    "        print(dict1)\n",
    "        print(dict2)\n",
    "\n",
    "        # compare length of dictionaries\n",
    "        self.assertEqual(len(dict1), len(dict2), \"The two code snippets did not produce the same number of sections.\")\n",
    "\n",
    "        for i in range(len(dict1)):\n",
    "            # 3. Compare number of expressions\n",
    "            self.assertEqual(len(dict1[i]['expressions']), len(dict2[i]['expressions']), \"The two code snippets did not produce the same number of expressions.\")\n",
    "        print(\"#Expression test case passed\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"#Expression test case failed\")\n",
    "        return 0\n",
    "\n",
    "def test_section_statements(self, gt_code, generated_code):\n",
    "    \"\"\"\n",
    "    Test the statements of two code snippets.\n",
    "    This function runs both code snippets, serializes the resulting sections,\n",
    "    and compares the number of statements in each section to ensure they match.\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      generated_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "        int: 1 if the test passes, 0 if it fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Run both snippets to get top-level sections\n",
    "        list_s1 = run_code_string(gt_code)\n",
    "        list_s2 = run_code_string(generated_code)\n",
    "\n",
    "        # 2. Serialize them\n",
    "        dict1 = [serialize_section(s) for s in list_s1]\n",
    "        dict2 = [serialize_section(s) for s in list_s2]\n",
    "\n",
    "        # compare length of dictionaries\n",
    "        self.assertEqual(len(dict1), len(dict2), \"The two code snippets did not produce the same number of sections.\")\n",
    "\n",
    "        for i in range(len(dict1)):\n",
    "            # 3. Compare number of statements \n",
    "            self.assertEqual(len(dict1[i]['statements']), len(dict2[i]['statements']), \"The two code snippets did not produce the same number of statements.\")\n",
    "        print(\"#Statement test case passed\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"#Statement test case failed\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test suite combining all tests\n",
    "def test_all(self, gt_code, generated_code):\n",
    "    \"\"\"\n",
    "    Run all tests on the generated code against the ground truth code.\n",
    "    This function runs the following tests:\n",
    "      - Section number\n",
    "      - Section name\n",
    "      - Section subsections\n",
    "      - Section expressions\n",
    "      - Section statements\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      generated_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "        int: The total number of passed tests.\n",
    "    \"\"\"\n",
    "    passed = 0\n",
    "    v=test_section_number(self, gt_code, generated_code)\n",
    "    passed += v\n",
    "    v=test_section_name(self, gt_code, generated_code)\n",
    "    passed += v\n",
    "    v=test_section_subsections(self, gt_code, generated_code)\n",
    "    passed += v\n",
    "    v=test_section_expressions(self, gt_code, generated_code)\n",
    "    passed += v\n",
    "    v=test_section_statements(self, gt_code, generated_code)\n",
    "    passed += v\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to define the total number of tests\n",
    "total_tests = 5\n",
    "\n",
    "def calculate_mean_accuracy_df(self, gt_df, gen_df):\n",
    "    \"\"\"\n",
    "    Calculate the mean accuracy of the generated code against the ground truth code.\n",
    "    This function runs all tests for each row in the dataframes and computes the mean accuracy.\n",
    "    Parameters:\n",
    "      gt_df (pd.DataFrame): DataFrame containing ground truth code.\n",
    "      gen_df (pd.DataFrame): DataFrame containing generated code.\n",
    "    Returns:\n",
    "        float: The mean accuracy of the generated code.\n",
    "    \"\"\"\n",
    "    passed=0\n",
    "    for i in range(len(gt_df)):\n",
    "        gt_code = gt_df.iloc[i]['GT Code']\n",
    "        generated_code = gen_df.iloc[i]['Generated Code']\n",
    "        passed+=test_all(unittest.TestCase(), gt_code, generated_code) / total_tests\n",
    "    mean_accuracy = passed/len(gt_df)\n",
    "    return mean_accuracy \n",
    "\n",
    "def list_passed_df(self, gt_df, gen_df):\n",
    "    \"\"\"\n",
    "    List the number of passed test cases for each row in the dataframes.\n",
    "    This function runs all tests for each row in the dataframes and counts the number of passed tests.\n",
    "    Parameters:\n",
    "      gt_df (pd.DataFrame): DataFrame containing ground truth code.\n",
    "      gen_df (pd.DataFrame): DataFrame containing generated code.\n",
    "    Returns:\n",
    "        list: A list of integers where each integer is the number of passed tests for that row.\n",
    "        int: The total count of rows where all tests passed.\n",
    "    \"\"\"\n",
    "    passed=[]\n",
    "    count_tota_passed = 0\n",
    "    for i in range(len(gt_df)):\n",
    "        gt_code = gt_df.iloc[i]['GT Code']\n",
    "        generated_code = gen_df.iloc[i]['Generated Code']\n",
    "        p=test_all(unittest.TestCase(), gt_code, generated_code)\n",
    "        passed.append(p)\n",
    "        if p==total_tests:\n",
    "            count_tota_passed += 1\n",
    "    return passed, count_tota_passed\n",
    "\n",
    "def create_map_sample(self, gt_code, gen_code):\n",
    "    \"\"\"\n",
    "    Create a map of test cases and their results for a single sample.\n",
    "    This function runs all tests on the generated code against the ground truth code\n",
    "    and creates a map with the results.\n",
    "    Parameters:\n",
    "      gt_code (str): The ground truth code snippet as a string.\n",
    "      gen_code (str): The generated code snippet as a string.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of all tests.\n",
    "    \"\"\"\n",
    "    map = {}\n",
    "    map[\"section_number\"] = test_section_number(unittest.TestCase(), gt_code, gen_code)\n",
    "    map[\"section_name\"] = test_section_name(unittest.TestCase(), gt_code, gen_code)\n",
    "    map[\"section_subsections\"] = test_section_subsections(unittest.TestCase(), gt_code, gen_code)\n",
    "    map[\"section_expressions\"] = test_section_expressions(unittest.TestCase(), gt_code, gen_code)\n",
    "    map[\"section_statements\"] = test_section_statements(unittest.TestCase(), gt_code, gen_code)\n",
    "    return map\n",
    "\n",
    "def create_map_df(self, gt_df, gen_df):\n",
    "    \"\"\"\n",
    "    Create a map of test cases and their results for each row in the dataframes.\n",
    "    This function runs all tests for each row in the dataframes and creates a map with the results.\n",
    "    Parameters:\n",
    "      gt_df (pd.DataFrame): DataFrame containing ground truth code.\n",
    "      gen_df (pd.DataFrame): DataFrame containing generated code.\n",
    "    Returns:\n",
    "        list: A list of dictionaries where each dictionary contains the results of all tests for that row.\n",
    "    \"\"\"\n",
    "    map = []\n",
    "    for i in range(len(gt_df)):\n",
    "        print(\"test case: \", i)\n",
    "        gt_code = gt_df.iloc[i]['GT Code']\n",
    "        gen_code = gen_df.iloc[i]['Generated Code']\n",
    "        map.append(create_map_sample(unittest.TestCase(), gt_code, gen_code))\n",
    "    return map\n",
    "\n",
    "def create_result_df(self, gt_df, gen_df):\n",
    "    \"\"\"\n",
    "    Create a result DataFrame containing the results of all tests for each row in the dataframes.\n",
    "    This function runs all tests for each row in the dataframes and creates a DataFrame with the results.\n",
    "    Parameters:\n",
    "      gt_df (pd.DataFrame): DataFrame containing ground truth code.\n",
    "      gen_df (pd.DataFrame): DataFrame containing generated code.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the results of all tests for each row.\n",
    "        float: The mean accuracy of the generated code.\n",
    "    \"\"\"\n",
    "    df_result = pd.DataFrame(create_map_df(unittest.TestCase(), gt_df, gen_df))\n",
    "    # add text and code columns to the result dataframe\n",
    "    df_result['Text'] = gt_df['text']\n",
    "    df_result['GT Code'] = gt_df['GT Code']\n",
    "    df_result['Generated Code'] = gen_df['Generated Code']\n",
    "    df_result['Total Passed'], count = list_passed_df(unittest.TestCase(), gt_df, gen_df)\n",
    "    mean_accuracy = calculate_mean_accuracy_df(unittest.TestCase(), gt_df, gen_df)\n",
    "    # move text and code columns to the front\n",
    "    cols = df_result.columns.tolist()\n",
    "    cols = cols[-4:] + cols[:-4]\n",
    "    df_result = df_result[cols]\n",
    "    print(\"Total passed test cases: \", count)\n",
    "    return df_result, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to your ground truth and generated code files\n",
    "gt_file = \"path/to/ground_truth.csv\"\n",
    "gen_file = \"path/to/generated_code.csv\"\n",
    "\n",
    "# Load the ground truth and generated code dataframes\n",
    "gt_df = pd.read_csv(gt_file)\n",
    "gen_df = pd.read_csv(gen_file)\n",
    "\n",
    "# Ensure the 'Generated Code' column is properly formatted\n",
    "gen_df['Generated Code'] = gen_df['Generated Code'].apply(\n",
    "    lambda x: x.split(\"```\")[1][7:].strip() if isinstance(x, str) and \"```\" in x else x\n",
    ")\n",
    "\n",
    "# Create the result DataFrame and calculate mean accuracy\n",
    "df_result, m_accuracy = create_result_df(unittest.TestCase(), gt_df, gen_df)\n",
    "print(\"Mean Accuracy: \", m_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result DataFrame to a CSV file\n",
    "df_result.to_csv(\"path/to/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
